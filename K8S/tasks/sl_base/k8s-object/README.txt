========================
- MAIN
- POD
- REPLICASET
- DEPLOYMENT 
- RESOURCES 
- ConfigMap 
- Secret 
- Volumes 
- Downward API
- initContainers 
- PROBE
- Service 
- HPA
==============MAIN======

# углубляться в описане документации
kubectl explain deployment
kubectl explain deployment.spec
kubectl explain deployment.spec.strategy

# применить все манифесты в папке 
kubectl apply -f <name_dir>/
kubectl delete -f <name_dir>/

# команда даёт расширенную информацию, в том числе IP адрес пода, имя ноды
kubectl get pod -o wide

# сходить внутрь контейнера шелл 
kubectl exec -it <name_pod> -- bash
kubectl exec -it <name_pod> -- ls /etc/nginx/conf.d/
kubectl exec -it <name_pod> -- cat /etc/nginx/conf.d/default.conf

# посмотреть сразу переменные окружения
kubectl exec -it <name_pod> -- env

# port-forward
kubectl port-forward <name_pod> 8080:80 &

# измение ресураса на лету 
kubectl patch deployment my-deployment --patch '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"100"},"limits":{"cpu":"100"}}}]}}}}'

#
kubectl run test --image=centosadmin/utils:0.3 -it bash
kubectl exec test -it bash

#
kubectl get node --show-labels
==============POD=======

# создание pod из файла:
kubectl create -f pod.yaml
kubectl get pod
kubectl get pod -l "app=my-app"

# посмотреть описание объекта
kubectl describe pod my-pod

# Удаление  
kubectl delete pod my-pod  	# с указанием типа и имени
kubectl delete -f pod.yaml 	# через файл, с помощью которого производили создание объекта
kubectl delete pod --all   	# подсказывают в чате, проверил работает

==============REPLICASET

# создаём (аплаим) репликасет:
kubectl apply -f replicaset.yaml
kubectl get replicaset
# DESIRED - сколько мы указывали создавать реплик
# CURRENT - сколько по факту есть
# READY - цифра 2 показывает что 2 пода сейчас работают и доступны.

# масштабирование
# 1 способ - открыть наш yaml файл и исправить там количество реплик и выполнить аплай изменений
# 2 способ - kubectl scale. Данным способом можно как добавлять так и убирать целевое количество реплик
# При скейлинге вниз одним из ключевых критериев является возраст пода, первыми будут удаляться самые молодые поды, т.е. созданные позже Пример со 2м способом:

kubectl scale --replicas 3 replicaset my-replicaset

==============DEPLOYMENT 

# Применяем наше описание и смотрим что получилось:
kubectl apply -f deployment.yaml
kubectl get deployment

# Обновляем приложение в рамках Deployment
# через правку yaml и apply (- целевой)
# командой kubectl set image deployment my-deployment '*=<image_name>:1.13'
# новый способ - редактирование ресурса на лету с помощью kubectl edit, например:
kubectl edit deployment my-deployment

# имеем 2 ReplicaSet, старый и текущий, с обновленной версией приложения
# старый ReplicaSet нужен, чтобы иметь возможность выполнить откат, это можно сделать командой:
kubectl rollout undo deployment my-deployment

# revisionHistoryLimit - отвечает за глубину хранения ReplicaSet'ов для наших Deployment'ов, то есть, количество версий, на которые мы сможем откатиться
# strategy - каким именно образом скейлить ReplicaSet' "Recreate" или "RollingUpdate"
# RollingUpdate
#     - maxSurge (от слова всплеск) - говорит о том, на сколько штук или процентов мы можем поднять количество подов при обновлении, относительно их желаемого количества, т.е. значения spec.replicas в нашем yaml описании
#       Например, у нас указано replicas: 2 и maxSurge: 1, таким образом в момент обновления у нас может быть 3 пода
#     - maxUnavailable - говорит о том, наскоько можно опустить количество реплик нашего приложения относительно желаемого значения
#       Например, у нас указано replicas: 2 и maxUnavailable: 1, таким образом в момент начала обновления 1 реплику сразу можно удалить 
# Recreate - стратегия, при которой сначала удаляются все старые поды, затем создаются новые.

==============RESOURCES=

# Limits - верхняя граница количество ресурсов, которое pod и наше приложение в нём (контейнер точнее?) может максимально использовать
# Requests - Количество ресурсов, которое резервируется для пода на ноде
# QoS зависит то, как kubernetes будет обрабатывать нехватку ресурсов:
#    - Best Effort - не ставим на наше приложение никаких лимитов и реквестов, поды будут удалены с ноды в первую очередь (и пересозданы где-то еще), чтобы освободить ресурсы для оставшихся подов с остальными QoS классами, чтобы они могли продолжать работать
#    - Burstable   - указаны реквесты, но не указаны лимиты, либо лимиты больше чем реквесты
#                    Поды данного класса во 2ю очередь будут удаляться с нод
#    - Guaranteed  - лимиты и реквесты равны

==============ConfigMap 
# для хранения конфигруций (секреты не храним)
kubectl apply -f configmap.yaml
kubectl get configmap

# увидеть, что должно быть записано в переменных окружения ConfigMap:
kubectl get configmap my-configmap-env -o yaml

==============Secret ===
# для хранения секретов
#  - generic - самый распространенный тип, обычно используется для токенов и логинов с паролями
#  - docker-registry - данные для авторизации в docker registry (секретом с заранее определённым списком ключей в массиве data)
#                      ключ, отвечающий за адрес репозитория
#                      ключи, отвечающий за логин, пароль и почту
#  - tls - предназначен для хранения сертификатов для шифрования данных, для HTTPS TLS протокола
#                      приватный ключ
#                      подписанный сертификат

kubectl get secret test -o yaml

==============Volumes ==
# HostPath - каталог, который находится на ноде и монтируем этот каталог внутрь контейнера
# EmptyDir - Создаёт временный диск и монтирует его внутрь контейнера
#            данные хранимые в emptyDir живут столько же, сколько и под, удалён под - удалены данные
#            Если контейнер внутри пода упадёт, сохранность данных это не затронет
# PV/PVC/Storage class - сущность которая смотрит на другой объект где описанны параметры подключения к Storage
#            нужен чтобы в deployment описать что необходим Storage, а в pvc.yml описываем уже параметры 
#      Persistent Volume  - Абстракция, которая создаётся и в которой записывается информация о том диске, 
#                           который был выдан нашему приложению, в PVC заявка, 
#                           PV хранится информация о том, какой диск в СХД был выдан нашему приложению, как правило, там ID диска
#      Storage class можем описать подключение к таким системам и указать данные для подключения, такие как:
#            адреса
#            логины/пароли/токены
#            различные другие настройки для взаимодействия с СХД


==============Downward API
# Документация:
# Expose Pod Information to Containers Through Environment Variables
https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/
# Expose Pod Information to Containers Through Files
https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/

# Позволяет передать приложению некоторые параметры манифестов как переменные окружения или как файлы
# Позволяет передать приложению различную информацию о том, где оно запущено, например:
#     - адрес узла
#     - название узла
#     - название неймспейса
#     - название пода
#     - адрес пода
#     - реквесты и лимиты, которые описаны в манифесте пода

- name: __POD_IP
  valueFrom: 
    fieldRef:    
      fieldPath: status.podIP

# В поле fieldPath мы указываем интересующие нас поля, которые мы хотим передать внутрь контейнера, это путь согласно yaml манифесту, структура обращения такая же как при вызове команды kubectl explain
# Некоторые значения берутся из манифеста, некоторые подставляет kubernetes, например, те что в секции status. О каждом имеет смысл читать отдельно в документации
# Аннотации (annotations) - кастомные поля для манифеста, применяются для данных, под которые еще не придумали специальных полей (к данной теме особо не относится)
==============initContainers  ==
# Обычный контейнер, который может запускаться перед запуском основного приложения, 
# обычно его применяют для каких-то доп. настроек
# таких контейнером может быть несколько, они выполняются по порядку описания в манифесте
# можно монтировать те же тома, что в основном контейнере
#                   пример - загрузка дампа БД из бэкапа
# можно запускать процессы от root, чтобы приложения в основных контейнерах работали от ограниченного пользователя,
# т.н. Rootless mode, это применяется для уменьшения потенциальной площади атаки в случае эксплуатации различных уязвимостей в контейнере
# После выполнения действия такие контейнеры останавливаются
# 
==============PROBE ====
# - Liveness Probe
#            Контроль за состоянием приложения во время его жизни
#            Исполняется постоянно, с заданной периодичностью
#            Если такая проверка будет провалена, то приложение (под) будет перезапущено
# - Readiness Probe 
#            Проверяет, готово ли приложение применять трафик
#            В случае неудачного выполнения приложение убирается из балансировки, соответственно, после этого в данный инстанс приложения перестанет идти трафик
#            Исполняется постоянно, с заданной периодичностью
#            При выводе kubectl get pod, в колонке READY выводится результат readinessProbe
# - Startup Probe 
#           Проверяет, запустилось ли приложение вообще
#           Исполняется при старте, остальные типы проверок начнут выполнятся после завершения проверки данного типа

==============Service ==
#   Ключевые моменты почти для всех сервисов
#       - Важно, чтобы селектор совпадал с лейблами подов
#       - Поды и сервисы должны находиться в одном неймспейсе
#  ClusterIP 
#       - Используется для того, чтобы наладить внутрикластерное взаимодействие частей приложения 
#         (например, связать фронтенд с бэкендом)
#       - с помощью данного сервиса мы можем пробрасывать приложение через kubectl port-forward на хост,
#         с которго запускается данная команда
#         kubectl port-forward service/my-service 10000:80 
#             пробросит 80 порт сервиса my-service на 10000 порт хоста, с которого запускается эта команда
#
#  NodePort 
#       - Позволяет опубликовать приложение наружу на определённом диапазоне портов, по умолчанию это 30000-32767
#
#  LoadBalancer 
#       - доступ извне кластера к нашему приложению
#
#  ExternalName 
#       - Позволяет создать алиас для DNS имени в качестве имени сервиса
#
#  ExternalIPs
#       - создается правило которое работает с IP адресом (данный адресс должен быть keepAlive VRRP)
#
#  Headless 
#
#       - Для такого сервиса не будет создан IP адрес, но будет создана DNS запись, 
#         которая будет отдавать IP адреса всех подов для этого сервиса
#         сервис используется со StatefulSet
#
#============== HPA =========
# Metric server - должны работать на кластере
# kubectl apply -f for-minikube-only-metrics-server/ -n kube-system
# проверка работы 
# kubectl top node
# Для работы HPA обязательным является наличие у Pod выставленных request. 
resources:
  requests:
    cpu: 100m
#
# Смотрим на HPA
kubectl get hpa

# Устанавливаем HPA
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=5

# Создаем нагрузку
kubectl run load-generator --image=busybox -- /bin/sh -c "while true; do wget -q -O- http://<servce\pod_name>; done"

# Спустя несколько минут количество Pod должно увеличиться до 5-ти.
kubectl get pod -w
#============================
#
#